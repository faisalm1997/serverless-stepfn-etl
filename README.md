# Scheduled ETL Pipeline

A serverless, scheduled ETL pipeline that extracts data from a public API, transforms it using AWS Glue, and makes it queryable via Athena.

## Repo Structure

```sh
scheduled-etl-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ API_GUIDE.md
â”‚   â”œâ”€â”€ GLUE_GUIDE.md
â”‚   â”œâ”€â”€ STEP_FUNCTIONS_GUIDE.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â”œâ”€â”€ s3.tf
â”‚   â”‚   â”œâ”€â”€ lambda_ingestion.tf
â”‚   â”‚   â”œâ”€â”€ glue.tf
â”‚   â”‚   â”œâ”€â”€ step_functions.tf
â”‚   â”‚   â”œâ”€â”€ iam.tf
â”‚   â”‚   â”œâ”€â”€ cloudwatch.tf
â”‚   â”‚   â””â”€â”€ eventbridge.tf
â”‚   â””â”€â”€ terragrunt/
â”‚       â”œâ”€â”€ terragrunt.hcl
â”‚       â””â”€â”€ dev/
â”‚           â””â”€â”€ terragrunt.hcl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lambda/
â”‚   â”‚   â”œâ”€â”€ api_ingestion/
â”‚   â”‚   â”‚   â”œâ”€â”€ lambda_handler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚   â”‚       â””â”€â”€ test_handler.py
â”‚   â”‚   â””â”€â”€ glue_trigger/
â”‚   â”‚       â””â”€â”€ lambda_handler.py
â”‚   â””â”€â”€ glue/
â”‚       â”œâ”€â”€ jobs/
â”‚       â”‚   â”œâ”€â”€ transform_job.py
â”‚       â”‚   â””â”€â”€ validate_job.py
â”‚       â”œâ”€â”€ scripts/
â”‚       â”‚   â”œâ”€â”€ data_quality.py
â”‚       â”‚   â””â”€â”€ transformations.py
â”‚       â””â”€â”€ tests/
â”‚           â””â”€â”€ test_transformations.py
â”œâ”€â”€ step_functions/
â”‚   â”œâ”€â”€ state_machine.json
â”‚   â””â”€â”€ state_machine.asl.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ package_lambda.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ test_api.sh
â”‚   â”œâ”€â”€ run_glue_local.sh
â”‚   â””â”€â”€ cleanup.sh
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_end_to_end.py
â”‚   â””â”€â”€ sample_data/
â”‚       â””â”€â”€ sample_api_response.json
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dev.yaml
â”‚   â”œâ”€â”€ prod.yaml
â”‚   â””â”€â”€ glue_job_config.json
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci-cd.yaml
```

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EventBridge â”‚ Daily 9 AM UTC
â”‚   Schedule  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Trigger
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step Functions  â”‚ Orchestrate workflow
â”‚  State Machine  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Lambda  â”‚ â”‚  Glue    â”‚
â”‚ API    â”‚ â”‚  Job     â”‚
â”‚Ingestionâ”‚ â”‚Transform â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Buckets       â”‚
â”‚  - Raw             â”‚
â”‚  - Processed       â”‚
â”‚  - Curated         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Glue   â”‚
    â”‚Crawler â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Athena â”‚
    â”‚ Query  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“š Learning Objectives

By building this project, you'll learn:

1. **API Integration**: Fetching data from public APIs with Lambda
2. **Step Functions**: Orchestrating multi-step workflows
3. **AWS Glue**: Serverless Spark for data transformation
4. **Glue Crawler**: Automated schema discovery
5. **Athena**: SQL queries on S3 data
6. **EventBridge**: Scheduled triggers (cron expressions)
7. **Data Quality**: Validation and error handling
8. **Infrastructure as Code**: Terraform/Terragrunt patterns

## ðŸŽ¯ Data Flow

1. **Extract**: Lambda fetches data from public API (e.g., OpenWeather, GitHub, CoinGecko)
2. **Raw Storage**: Store raw JSON in S3 `raw/` prefix with date partition
3. **Transform**: Glue job reads raw data, cleans, validates, and transforms
4. **Curated Storage**: Write Parquet files to S3 `curated/` with partitions
5. **Catalog**: Glue Crawler updates Data Catalog
6. **Query**: Athena allows SQL queries on the curated data

## ðŸš€ Getting Started

### Prerequisites

- AWS Account with Admin access
- AWS CLI configured
- Terraform >= 1.5
- Python 3.12
- Docker (for local Glue testing)

### Quick Start

```bash
1. Clone the repository

git clone <your-repo>
cd scheduled-etl-pipeline

2. Choose your API

Edit config/dev.yaml and set your API choice:
- openweather (weather data)
- github (repository stats)
- coingecko (cryptocurrency prices)

3. Package Lambda functions
./scripts/package_lambda.sh

4. Deploy infrastructure (terragrunt used to manage infrastructure)

cd infrastructure/terragrunt/dev
terragrunt init
terragrunt apply

5. Test the pipeline

cd ../../../
./scripts/test_api.sh

6. Trigger Step Function manually (first time)

aws stepfunctions start-execution \
  --state-machine-arn $(terragrunt output -raw step_function_arn) \
  --name "manual-test-$(date +%s)"

7. Query data in Athena
Go to AWS Console > Athena
Run: SELECT * FROM curated_data LIMIT 10;

```

## Step-by-Step Build Guide

### Phase 1: API Ingestion
- [ ] Create Lambda function to fetch API data
- [ ] Store raw JSON in S3 with date partitions
- [ ] Add error handling and retries
- [ ] Test with sample API

**Guide**: See `docs/API_GUIDE.md`

### Phase 2: Glue ETL
- [ ] Write Glue job to read raw JSON
- [ ] Transform data (clean, validate, enrich)
- [ ] Write Parquet to curated bucket
- [ ] Test locally with Docker

**Guide**: See `docs/GLUE_GUIDE.md`

### Phase 3: Orchestration
- [ ] Design Step Functions state machine
- [ ] Add error handling and retries
- [ ] Connect Lambda and Glue job
- [ ] Add success/failure notifications

**Guide**: See `docs/STEP_FUNCTIONS_GUIDE.md`

### Phase 4: Automation
- [ ] Add EventBridge schedule
- [ ] Configure Glue Crawler
- [ ] Set up Athena database
- [ ] Create CloudWatch dashboards

## ðŸ§ª Testing

```bash
# Test Lambda locally
cd src/lambda/api_ingestion
python -m pytest tests/

# Test Glue job locally (requires Docker)
./scripts/run_glue_local.sh

# Integration test
python tests/integration/test_end_to_end.py
```

## Monitoring

- **CloudWatch Logs**: `/aws/lambda/api-ingestion`, `/aws-glue/jobs/transform-job`
- **Step Functions**: Execution history in AWS Console
- **Glue Job Metrics**: Duration, DPU hours, records processed
- **Cost Tracking**: Tagged resources in Cost Explorer

## Cost Estimate

**Daily run (assuming small dataset):**
- Lambda: ~$0.01/day
- Glue: ~$0.44/hour (only runs ~5 min) = ~$0.04/day
- S3: ~$0.02/day
- Step Functions: ~$0.0001/day
- **Total Estimate: ~$0.07/day or ~$2/month**

## Technologies

- **AWS Services**: Lambda, Glue, S3, Step Functions, EventBridge, Athena, CloudWatch
- **IaC**: Terraform, Terragrunt
- **Languages**: Python 3.12, PySpark
- **Testing**: pytest, moto (AWS mocking)

## Resources

- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)
- [Step Functions Tutorial](https://docs.aws.amazon.com/step-functions/)
- [PySpark Guide](https://spark.apache.org/docs/latest/api/python/)

## Contributing

This is a learning project. Feel free to:
- Add new API sources
- Improve data quality checks
- Add more transformation logic
- Enhance monitoring